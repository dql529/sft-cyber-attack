# save_train_val_csv_fixed.py
import pandas as pd
import os
import sys
import csv
from sklearn.model_selection import train_test_split
from collections import Counter

# --- è·¯å¾„é…ç½® ---
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)
from config import FILTERED_DATA_CSV, PROMPT_TRAIN_CSV, PROMPT_VAL_CSV, LABEL_MAP


def resolve_path(path_str):
    if isinstance(path_str, str) and (
        path_str.startswith("./") or path_str.startswith("../")
    ):
        return os.path.join(project_root, path_str)
    return path_str


FILTERED_DATA_CSV = resolve_path(FILTERED_DATA_CSV)
PROMPT_TRAIN_CSV = resolve_path(PROMPT_TRAIN_CSV)
PROMPT_VAL_CSV = resolve_path(PROMPT_VAL_CSV)
# --- è·¯å¾„é…ç½®ç»“æŸ ---

df = pd.read_csv(FILTERED_DATA_CSV)
print(f"Loaded {len(df)} rows from {FILTERED_DATA_CSV}")

# --- å¿…è¦åˆ—æ£€æŸ¥ï¼Œé¿å… KeyError ---
required_cols = {
    "proto",
    "state",
    "service",
    "dur",
    "sbytes",
    "dbytes",
    "spkts",
    "dpkts",
    "sload",
    "dload",
    "tcprtt",
    "ct_srv_src",
    "attack_cat",
}
missing = required_cols - set(df.columns)
if missing:
    raise RuntimeError(f"Missing required columns in input CSV: {missing}")

print("ğŸ”§ Engineering new semantic features from numerical data...")

# ====================================================================
# åˆ†ç®±è¦å¤„ç†çš„åˆ—ï¼ˆå’Œä½ åŸæ¥ä¸€è‡´ï¼‰
cols_to_categorize = [
    "sbytes",
    "dbytes",
    "spkts",
    "dpkts",
    "dur",
    "sload",
    "dload",
    "tcprtt",
]
# å¯é€‰çš„æ ‡ç­¾é˜¶å±‚ï¼ˆæœ€å¤šï¼‰
base_labels = ["low", "medium", "high", "very_high"]
# ====================================================================

for col in cols_to_categorize:
    # ç¡®ä¿ numeric
    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
    non_zero_data = df[df[col] > 0][col]
    if not non_zero_data.empty:
        try:
            # å…ˆç”¨ qcut è·å– binsï¼ˆå¯èƒ½ä¼šå› ä¸ºé‡å¤å€¼è€Œå‡å°‘ç®±æ•°ï¼‰
            _, bins = pd.qcut(
                non_zero_data, q=len(base_labels), retbins=True, duplicates="drop"
            )
            n_bins = len(bins) - 1
            real_labels = base_labels[:n_bins]  # è‹¥ qcut æŠŠç®±æ•°é™äº†ï¼Œåˆ™ labels æˆªæ–­åŒ¹é…
            # åº”ç”¨åˆ†ç®±åˆ°æ•´åˆ—
            df[f"{col}_cat"] = pd.cut(
                df[col], bins=bins, labels=real_labels, include_lowest=True
            )
            # æŠŠç¼ºå¤±ï¼ˆ0 æˆ– NaNï¼‰è®¾ä¸º 'zero' å¹¶æŠŠåˆ—è®¾æˆ categoricalï¼Œä¿è¯ dtype ä¸€è‡´
            cat_dtype = pd.CategoricalDtype(
                categories=real_labels + ["zero"], ordered=True
            )
            df[f"{col}_cat"] = (
                df[f"{col}_cat"]
                .cat.add_categories("zero")
                .fillna("zero")
                .astype(cat_dtype)
            )
            print(f"  - Categorized '{col}' into {n_bins} bins + zero.")
        except Exception as e:
            # å…œåº•ï¼šå¦‚æœ qcut/pd.cut éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°å€¼çš„å­—ç¬¦ä¸²åŒ–ç±»åˆ«ï¼ˆå°æ¦‚ç‡ï¼‰
            print(f"  - Could not categorize '{col}': {e}. Fallback to coarse bins.")
            # ç®€å•åˆ†ä¸º zero / positive ä¸¤ç±»
            cat_dtype = pd.CategoricalDtype(
                categories=["zero", "positive"], ordered=False
            )
            df[f"{col}_cat"] = pd.Categorical(
                ["positive" if v > 0 else "zero" for v in df[col]], dtype=cat_dtype
            )
    else:
        # å…¨ä¸º 0 çš„åˆ—ï¼Œç›´æ¥ç»Ÿä¸€ä¸º categorical 'zero'
        cat_dtype = pd.CategoricalDtype(categories=base_labels + ["zero"], ordered=True)
        df[f"{col}_cat"] = pd.Categorical(["zero"] * len(df), dtype=cat_dtype)
        print(f"  - Column '{col}' contains only zeros; set '{col}_cat' = 'zero'.")

# LABEL æ˜ å°„ï¼ˆå‡è®¾ config.LABEL_MAP æ˜¯ {int: str}ï¼‰
label2id = {v: k for k, v in LABEL_MAP.items()}

# attack_cat å¤„ç†ï¼šå»ä¸¤ç«¯ç©ºç™½å¹¶ç»Ÿä¸€å¤§å°å†™ï¼ˆè‹¥æœ‰ NaN ä¿æŒï¼‰
df["attack_cat"] = df["attack_cat"].astype(str).str.strip()
df["label"] = df["attack_cat"].map(label2id)

num_null_labels = df["label"].isnull().sum()
if num_null_labels > 0:
    print(f"  - è­¦å‘Šï¼šä¸¢å¼ƒ {num_null_labels} è¡Œæ— æ³•æ˜ å°„æ”»å‡»ç±»åˆ«çš„æ•°æ®ã€‚")
    df = df.dropna(subset=["label"]).copy()

df["label"] = df["label"].astype(int)


# ========== æ„é€  prompt çš„å‡½æ•°ï¼ˆæ›´é²æ£’ï¼‰ ==========
def safe_get(row, key, default="-"):
    v = row.get(key, default)
    if pd.isna(v):
        return default
    return v


def build_prompt(row):
    class_labels = "[Backdoor, DoS, Exploits, Normal, Reconnaissance, Shellcode, Worms]"

    proto = safe_get(row, "proto", "unknown")
    state = safe_get(row, "state", "unknown")
    dur_cat = safe_get(row, "dur_cat", "zero")

    description_parts = [
        f"A {proto} connection with {state} state had a {dur_cat} duration."
    ]

    service = safe_get(row, "service", "-")
    if service != "-" and service != "unknown":
        description_parts.append(f"The connection service was identified as {service}.")
    else:
        description_parts.append("The connection service was not specified.")

    # packet and bytes categories (ä½¿ç”¨ *_cat åˆ—)
    s_pkts = safe_get(row, "spkts_cat", "zero")
    s_bytes = safe_get(row, "sbytes_cat", "zero")
    d_pkts = safe_get(row, "dpkts_cat", "zero")
    d_bytes = safe_get(row, "dbytes_cat", "zero")
    description_parts.append(
        f"It involved {s_pkts} source packets (total size: {s_bytes}) and {d_pkts} destination packets (total size: {d_bytes})."
    )

    sload = safe_get(row, "sload_cat", "zero")
    dload = safe_get(row, "dload_cat", "zero")
    description_parts.append(
        f"The source load was {sload} and the destination load was {dload}."
    )

    # å¦‚æœåŸå§‹ tcprtt æ•°å€¼å¤§äº0ï¼Œæ·»åŠ  RTT æè¿°ï¼ˆæ³¨æ„ä½¿ç”¨åŸå§‹æ•°å€¼åˆ¤æ–­ï¼‰
    try:
        tcprtt_val = float(row.get("tcprtt", 0) or 0)
    except Exception:
        tcprtt_val = 0
    if proto == "tcp" and tcprtt_val > 0:
        tcprtt_cat = safe_get(row, "tcprtt_cat", "zero")
        description_parts.append(f"The TCP round-trip time was {tcprtt_cat}.")

    # è®¿é—®å†å²æ¬¡æ•°ï¼ˆé¿å…éæ•°å€¼ï¼‰
    ct_srv_src = int(row.get("ct_srv_src", 0) or 0)
    description_parts.append(
        f"The source has connected to this same service {ct_srv_src} times before."
    )

    traffic_description = " ".join(description_parts)

    # ä½¿ç”¨ä¸‰å¼•å·æ„å»º promptï¼ˆæ³¨æ„ä¿ç•™ç»“æ„ï¼Œä½†ä¸è¦åœ¨ text å†…éƒ¨åŠ å…¥å¤šä½™çš„å¼•å·ï¼‰
    prompt = f"""You are an expert cybersecurity analyst. Your task is to classify network 
      traffic into ONE of the following categories: {class_labels}.

   Follow these rules strictly:
   1. Analyze the traffic description.
   2. Your response MUST be ONLY the category name.
   3. Do not add any explanation, punctuation, or other text.

   ---
   [Example]
   Input: A tcp connection with FIN state had a normal duration. It involved low source packets
      (total size: low) and zero destination packets (total size: zero). The source previously accessed
      the service 1 times.
   Answer: Normal
   ---
   [Task]
   Input: {traffic_description}
   Answer:"""

    return prompt


# ----------------- å®‰å…¨ç‰ˆï¼šç”Ÿæˆ prompt å¹¶è®°å½•é”™è¯¯ -----------------
from tqdm import tqdm
import traceback
import time

# å…ˆç¡®ä¿ ct_srv_src æ˜¯æ•´æ•°ï¼ˆé¿å… int("nan") æŠ¥é”™ç­‰ï¼‰
if "ct_srv_src" in df.columns:
    df["ct_srv_src"] = (
        pd.to_numeric(df["ct_srv_src"], errors="coerce").fillna(0).astype(int)
    )
else:
    df["ct_srv_src"] = 0

# ä¹Ÿç»Ÿä¸€ service å­—æ®µä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ None/float å¯¼è‡´æ‹¼æ¥å¼‚å¸¸
if "service" in df.columns:
    df["service"] = df["service"].astype(str).fillna("-")
else:
    df["service"] = "-"

# å‡†å¤‡å®¹å™¨ä¸é”™è¯¯è®°å½•
texts = []
errors = []  # list of tuples (index, error_str, tb_short)
start_time = time.time()

# ç”¨è¿­ä»£å¹¶æ•è·å¼‚å¸¸çš„æ–¹å¼é€è¡Œç”Ÿæˆï¼ˆæ¯” apply æ›´æ˜“è°ƒè¯•ï¼‰
for i, (idx, row) in enumerate(
    tqdm(df.iterrows(), total=len(df), desc="Building prompts")
):
    try:
        txt = build_prompt(row)
        # é˜²å®ˆï¼šä¿è¯æ˜¯å­—ç¬¦ä¸²
        if not isinstance(txt, str):
            txt = str(txt)
        texts.append(txt)
    except Exception as e:
        # æ•è·å®Œæ•´ traceï¼Œè®°å½•åˆ° errors åˆ—è¡¨å¹¶ç»§ç»­
        tb = traceback.format_exc(limit=5)
        errors.append((idx, str(e), tb))
        texts.append("")  # ç”¨ç©ºå­—ç¬¦ä¸²å ä½ï¼Œåé¢å¯ç­›æ‰æˆ–æ£€æŸ¥
        # ç°åœºæ‰“å°å‰å‡ æ¡é”™è¯¯ä»¥ä¾¿å¿«é€Ÿå®šä½
        if len(errors) <= 10:
            print(f"\n--- build_prompt ERROR at row idx={idx} (sample) ---")
            print("Error:", e)
            print(tb)
            print(
                "Row sample keys:",
                {
                    k: row.get(k)
                    for k in [
                        "proto",
                        "state",
                        "service",
                        "dur",
                        "ct_srv_src",
                        "attack_cat",
                    ]
                },
            )
        # å¯é€‰ï¼šå¦‚æœé”™è¯¯å¾ˆé¢‘ç¹å¹¶ä¸”ä½ æƒ³ä¸­æ–­ä»¥ç«‹åˆ»ä¿®å¤ï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Š
        # if len(errors) > 200: break

elapsed = time.time() - start_time
print(
    f"\nPrompt generation finished in {elapsed:.1f}s. Total rows: {len(df)}, errors: {len(errors)}"
)

# å°†ç”Ÿæˆæ–‡æœ¬èµ‹å› DataFrame
df["text"] = texts

# å¦‚æœæœ‰é”™è¯¯ï¼Œå†™ä¸€ä¸ªé”™è¯¯æ—¥å¿—æ–‡ä»¶ï¼ˆåŒ…å«è¡Œç´¢å¼•ä¸ tracebackï¼‰
if errors:
    os.makedirs(os.path.dirname(PROMPT_TRAIN_CSV), exist_ok=True)
    err_log_path = os.path.join(
        os.path.dirname(PROMPT_TRAIN_CSV), "prompt_generation_errors.log"
    )
    with open(err_log_path, "w", encoding="utf-8") as f:
        f.write(f"Prompt generation errors: {len(errors)}\n\n")
        for idx, errstr, tb in errors:
            f.write(f"ROW_INDEX: {idx}\nERROR: {errstr}\nTRACEBACK:\n{tb}\n{'-'*60}\n")
    print(f"Errors logged to: {err_log_path}")
    # æ‰“å°å‰ 5 ä¸ªæœ‰é—®é¢˜è¡Œçš„ç´¢å¼•ï¼Œä¾¿äºä½ ç›´æ¥å®šä½æ•°æ®æº
    print("Sample error rows indices:", [e[0] for e in errors[:10]])

# ç®€å•æ£€æŸ¥ï¼šæœ‰å¤šå°‘æ˜¯ç©º prompt
num_empty = df["text"].astype(str).str.strip().eq("").sum()
print(f"Empty/failed prompts: {num_empty} / {len(df)}")

# å¦‚æœå¤§é‡ç©ºï¼Œå¯ä»¥æŠŠå‡ºé—®é¢˜çš„è¡Œå¯¼å‡ºä¾›ä½ ç›´æ¥æŸ¥çœ‹
if num_empty > 0:
    bad_idx = df[df["text"].astype(str).str.strip() == ""].index
    sample_bad = df.loc[
        bad_idx[:50], ["proto", "state", "service", "dur", "ct_srv_src", "attack_cat"]
    ]
    sample_bad_path = os.path.join(
        os.path.dirname(PROMPT_TRAIN_CSV), "prompt_failed_sample.csv"
    )
    sample_bad.to_csv(sample_bad_path, index=True, quoting=csv.QUOTE_ALL)
    print(f"Saved sample of failed rows to: {sample_bad_path}")

# ----------------- ä¹‹åç»§ç»­åŸæœ‰åˆ‡åˆ†ä¸ä¿å­˜æµç¨‹ -----------------
# ========== åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆæ£€æŸ¥ stratify æ¡ä»¶ï¼‰ ==========
label_counts = Counter(df["label"].tolist())
print("Label distribution (after filtering):", dict(label_counts))

min_count = min(label_counts.values()) if label_counts else 0
if min_count < 2:
    print(
        "  - è­¦å‘Šï¼šæŸäº›ç±»åˆ«æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•åš stratified split. å°†ä½¿ç”¨æ—  stratify çš„éšæœºåˆ‡åˆ†ã€‚"
    )
    stratify_arg = None
else:
    stratify_arg = df["label"]

train_df, val_df = train_test_split(
    df[["text", "label"]],
    test_size=0.2,
    random_state=42,
    stratify=stratify_arg,
)

# ========== ä¿å­˜ CSVï¼ˆå¼ºåˆ¶å¼•ç”¨æ‰€æœ‰å­—æ®µï¼Œé¿å… prompt å†…éƒ¨çš„é€—å·/æ¢è¡Œç ´å CSV ç»“æ„ï¼‰ ==========
os.makedirs(os.path.dirname(PROMPT_TRAIN_CSV), exist_ok=True)
train_df.to_csv(PROMPT_TRAIN_CSV, index=False, quoting=csv.QUOTE_ALL)
val_df.to_csv(PROMPT_VAL_CSV, index=False, quoting=csv.QUOTE_ALL)

print("ä¿å­˜æ–°çš„ã€ä¿¡æ¯æ›´ä¸°å¯Œçš„è¯­ä¹‰åŒ–Prompt CSVæ–‡ä»¶ï¼š")
print(f"- {PROMPT_TRAIN_CSV}  (rows: {len(train_df)})")
print(f"- {PROMPT_VAL_CSV}    (rows: {len(val_df)})")
