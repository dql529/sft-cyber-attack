# -*- coding: utf-8 -*-
# ========= Windows/CUDA ç¨³å®šå¼€å…³ =========
# finetune
import os

os.environ["TORCHINDUCTOR_DISABLE"] = "1"
os.environ["TORCHDYNAMO_DISABLE"] = "1"

import sys, shutil, json, hashlib, random
import pandas as pd
from datasets import Dataset, load_from_disk

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    default_data_collator,
)
from peft import LoraConfig, get_peft_model, TaskType
import torch

# ========= è·¯å¾„ & é…ç½® =========
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)
from config import *  # å¤ç”¨ä½ çš„å¸¸é‡


def resolve_path(p):
    if isinstance(p, str) and (p.startswith("./") or p.startswith("../")):
        return os.path.join(project_root, p)
    return p


BASE_MODEL_PATH = resolve_path(BASE_MODEL_PATH)
PROMPT_TRAIN_CSV = resolve_path(PROMPT_TRAIN_CSV)
PROMPT_VAL_CSV = resolve_path(PROMPT_VAL_CSV)
TOKENIZED_TRAIN_PATH = resolve_path(TOKENIZED_TRAIN_PATH)
TOKENIZED_VAL_PATH = resolve_path(TOKENIZED_VAL_PATH)
OUTPUT_DIR = resolve_path(OUTPUT_DIR)

# ========= Tokenizer & æ¨¡å‹ =========
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.truncation_side = "left"  # â˜… ä¿ç•™å°¾éƒ¨ï¼ˆå« Answer: <LABEL>ï¼‰

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH, trust_remote_code=True, attn_implementation="eager"
)

# ååä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
torch.backends.cuda.matmul.allow_tf32 = True
try:
    torch.set_float32_matmul_precision("high")
except Exception:
    pass

# ========= LoRA é…ç½® & æ³¨å…¥ =========
lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=LORA_TARGET_MODULES,  # ç¡®ä¿è‡³å°‘åŒ…å« q_proj/k_proj/v_proj/o_proj
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)
model = get_peft_model(model, lora_config)

# â˜… ä¸ gradient checkpointing é…å¥—çš„å…³é”®ä¸‰è¡Œ
model.config.use_cache = False
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

model.print_trainable_parameters()


# ========= é¢„å¤„ç†ï¼šåªç›‘ç£â€œç­”æ¡ˆç‰‡æ®µâ€ =========
# CSV çš„ text å·²åŒ…å« "... Answer:"ï¼›æˆ‘ä»¬åªè®­ç»ƒ " <LABEL>" çš„ <LABEL> token
def preprocess_data(examples, max_len=256):
    batch = {k: [] for k in ["input_ids", "attention_mask", "labels"]}
    for text, label_id in zip(examples["text"], examples["label"]):
        y = LABEL_MAP[label_id]  # ä¾‹å¦‚ "Exploits"
        prompt_part = text  # å« Answer: å‰ç¼€
        answer_part = " " + y

        prompt_ids = tokenizer(prompt_part, add_special_tokens=False)["input_ids"]
        answer_ids = tokenizer(answer_part, add_special_tokens=False)["input_ids"]

        input_ids = prompt_ids + answer_ids
        labels = [-100] * len(prompt_ids) + answer_ids  # ä»…ç­”æ¡ˆå‚ä¸æŸå¤±

        # æˆªæ–­åˆ° max_lenï¼ˆä¿å°¾éƒ¨ï¼‰
        if len(input_ids) > max_len:
            input_ids = input_ids[-max_len:]
            labels = labels[-max_len:]

        # å·¦ä¾§ PADï¼›attention_mask: PAD=0, éPAD=1
        pad_len = max_len - len(input_ids)
        if pad_len > 0:
            input_ids = [tokenizer.pad_token_id] * pad_len + input_ids
            labels = [-100] * pad_len + labels
        attn = [0] * pad_len + [1] * (len(input_ids) - pad_len)

        batch["input_ids"].append(input_ids)
        batch["attention_mask"].append(attn)
        batch["labels"].append(labels)
    return batch


# ========= ç¼“å­˜ç­¾åï¼ˆè‡ªåŠ¨è¯†åˆ«é¢„å¤„ç†/æ¨¡æ¿å˜åŒ–ï¼‰ =========
PREPROC_SIGNATURE = {
    "max_len": 256,
    "truncation_side": "left",
    "supervise": "answer_only",
    "collator": "default_data_collator",
    "prompt_version": "v2_class_list_only_one",  # ä½ æ”¹æ¨¡æ¿æ—¶æ”¹è¿™ä¸ªå­—ç¬¦ä¸²
}


def _sig_hex(d: dict) -> str:
    return hashlib.md5(json.dumps(d, sort_keys=True).encode()).hexdigest()


PREPROC_SIG_HEX = _sig_hex(PREPROC_SIGNATURE)


def build_or_load_datasets():
    sig_file = os.path.join(
        os.path.dirname(TOKENIZED_TRAIN_PATH), "tokenized_signature.json"
    )

    if (
        os.path.exists(TOKENIZED_TRAIN_PATH)
        and os.path.exists(TOKENIZED_VAL_PATH)
        and os.path.exists(sig_file)
    ):
        try:
            with open(sig_file, "r", encoding="utf-8") as f:
                saved = json.load(f)
            if saved.get("sig_hex") == PREPROC_SIG_HEX:
                print("âœ… ç¼“å­˜ç­¾åä¸€è‡´ï¼ŒåŠ è½½ tokenized æ•°æ®é›†...")
                return load_from_disk(TOKENIZED_TRAIN_PATH), load_from_disk(
                    TOKENIZED_VAL_PATH
                )
            else:
                print("âš ï¸ ç¼“å­˜ç­¾åä¸ä¸€è‡´ï¼Œå°†é‡å»º tokenized æ•°æ®...")
        except Exception:
            print("âš ï¸ è¯»å–ç¼“å­˜ç­¾åå¤±è´¥ï¼Œå°†é‡å»º tokenized æ•°æ®...")

    print("ğŸ”„ é‡å»º tokenize æ•°æ®...")
    for p in [TOKENIZED_TRAIN_PATH, TOKENIZED_VAL_PATH]:
        if os.path.exists(p):
            shutil.rmtree(p, ignore_errors=True)

    train_df = pd.read_csv(PROMPT_TRAIN_CSV)
    val_df = pd.read_csv(PROMPT_VAL_CSV)
    train_ds = Dataset.from_pandas(train_df)
    val_ds = Dataset.from_pandas(val_df)

    train_ds = train_ds.map(
        lambda ex: preprocess_data(ex, max_len=PREPROC_SIGNATURE["max_len"]),
        batched=True,
        remove_columns=["text", "label"],
    )
    val_ds = val_ds.map(
        lambda ex: preprocess_data(ex, max_len=PREPROC_SIGNATURE["max_len"]),
        batched=True,
        remove_columns=["text", "label"],
    )

    os.makedirs(os.path.dirname(TOKENIZED_TRAIN_PATH), exist_ok=True)
    train_ds.save_to_disk(TOKENIZED_TRAIN_PATH)
    val_ds.save_to_disk(TOKENIZED_VAL_PATH)

    with open(sig_file, "w", encoding="utf-8") as f:
        json.dump(
            {"sig_hex": PREPROC_SIG_HEX, "detail": PREPROC_SIGNATURE},
            f,
            ensure_ascii=False,
            indent=2,
        )
    print("ğŸ’¾ å·²ä¿å­˜æ–° tokenized æ•°æ®åŠç­¾åã€‚")
    return train_ds, val_ds


# ========= Sanity Checks =========
def debug_tail(ds, idx=0, tail_tokens=60):
    ids = ds[idx]["input_ids"]
    labs = ds[idx]["labels"]
    seq = [t for t in ids if t != tokenizer.pad_token_id]
    tail_txt = tokenizer.decode(seq[-tail_tokens:]) if seq else ""
    tail_labs = labs[-tail_tokens:]
    print("TAIL TEXT:", tail_txt.replace("\n", "\\n"))
    print("TAIL LABEL IDs (last {}):".format(tail_tokens), tail_labs)
    assert any(x != -100 for x in tail_labs), "âŒ å°¾éƒ¨æ²¡æœ‰ç›‘ç£ï¼ˆlabels å…¨æ˜¯ -100ï¼‰"


def debug_forward(ds):
    # ä¸´æ—¶å…³é—­ checkpointing åšä¸€æ¬¡æ¢¯åº¦è¿é€šæ€§æ£€æŸ¥
    try:
        model.gradient_checkpointing_disable()
    except Exception:
        pass

    model.train()
    batch = {k: torch.tensor(v[:2]).to(model.device) for k, v in ds[:2].items()}
    out = model(**batch)
    loss = out.loss
    loss.backward()
    print("forward loss:", float(loss))

    grad_ok = any(
        p.requires_grad and p.grad is not None for _, p in model.named_parameters()
    )
    assert grad_ok, "âŒ æœªè§åˆ°å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦â€”â€”LoRA/é¢„å¤„ç†å¯èƒ½æœ‰é—®é¢˜"

    # æ¢å¤ checkpointingï¼ˆè®­ç»ƒæ—¶å¼€å¯ï¼‰
    try:
        model.gradient_checkpointing_enable()
        model.enable_input_require_grads()
    except Exception:
        pass


# ========= ç‰ˆæœ¬å…¼å®¹çš„ TrainingArguments æ„é€  =========
def build_training_args(quick=True):
    lr = max(LEARNING_RATE, 2e-4)  # LoRA å°æ¨¡å‹å»ºè®® >= 2e-4
    base = dict(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=EVAL_BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,
        learning_rate=lr,
        warmup_ratio=0.1,
        fp16=True,
        gradient_checkpointing=True,
        report_to="none",
    )
    try:
        if quick:
            print("\nâ„¹ï¸  NOTE: Running in quick validation mode (max_steps=200).")
            return TrainingArguments(
                **base,
                max_steps=200,
                logging_steps=20,
                save_strategy="no",
                evaluation_strategy="no",
            )
        else:
            print("\nâ„¹ï¸  NOTE: Full training mode (3 epochs).")
            return TrainingArguments(
                **base,
                num_train_epochs=3,
                weight_decay=0.01,
                logging_steps=100,
                save_strategy="epoch",
                evaluation_strategy="no",
            )
    except TypeError:
        # å…¼å®¹éå¸¸è€çš„ transformersï¼ˆæ—  evaluation/save_strategy ç­‰ï¼‰
        base.pop("report_to", None)
        if quick:
            return TrainingArguments(**base, max_steps=200, logging_steps=20)
        else:
            return TrainingArguments(**base, num_train_epochs=3, logging_steps=100)


# ========= è®­ç»ƒå…¥å£ =========
if __name__ == "__main__":
    random.seed(42)

    train_dataset, val_dataset = build_or_load_datasets()

    # â€”â€”â€” Sanity Check: 10 åˆ†é’Ÿå†…èƒ½åšå®Œ â€”â€”â€”
    print("\n[Sanity A] æ ·æœ¬å°¾éƒ¨æ£€æŸ¥ï¼ˆå¿…é¡»å« 'Answer: <LABEL>' ä¸” labels!= -100ï¼‰")
    debug_tail(train_dataset, idx=0, tail_tokens=60)

    print("\n[Sanity B] å•æ­¥å‰å‘+åä¼ ï¼ˆloss & æ¢¯åº¦ï¼‰")
    debug_forward(train_dataset)

    # å¿«éªŒ / å…¨é‡
    QUICK_VALIDATION = True
    args = build_training_args(quick=QUICK_VALIDATION)

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,  # æ–°ç‰ˆä¼šæç¤ºç”¨ processing_classï¼Œæ— ä¼¤å¤§é›…
        data_collator=default_data_collator,  # â˜… å…³é”®ï¼šä¸è¦ç”¨ MLM collator
    )

    print("\n" + "=" * 30)
    print("ğŸš€ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ...")
    print("=" * 30)
    trainer.train()

    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ï¼š", OUTPUT_DIR)
