# predict_baseline.py
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sklearn.metrics import classification_report
import re
from config import BASE_MODEL_PATH, PROMPT_VAL_CSV, LABEL_MAP, ID2LABEL

# âœ… è·¯å¾„é…ç½®
model_name = BASE_MODEL_PATH
val_file = PROMPT_VAL_CSV

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… åŠ è½½æ¨¡å‹å’Œ tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(
    device
)
model.eval()

# âœ… åŠ è½½éªŒè¯é›†æ•°æ®
df = pd.read_csv(val_file)


# âœ… æ¨ç†å‡½æ•°ï¼šç”Ÿæˆå¹¶æå–é¢„æµ‹ç±»åˆ«
def extract_label(text):
    match = re.search(r"Answer:\s*(\w+)", text)
    return match.group(1) if match else ""


y_true = []
y_pred = []

print("ğŸ” æ­£åœ¨è¿›è¡Œ zero-shot æ¨ç†...")

for _, row in df.iterrows():
    prompt = row["text"]  # å« Answer:
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=5,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    predicted_label = extract_label(full_output).strip()

    if predicted_label in ID2LABEL:
        y_pred.append(ID2LABEL[predicted_label])
        y_true.append(row["label"])
    else:
        # æœªè¯†åˆ«ç±»åˆ«æ—¶æ ‡è®°ä¸ºé”™è¯¯ï¼ˆå¯è®¾ä¸º -1ï¼‰
        y_pred.append(-1)
        y_true.append(row["label"])

# âœ… è¯„ä¼°
print("\nğŸ“Š åˆ†ç±»è¯„ä¼°æŠ¥å‘Šï¼ˆæœªå¾®è°ƒ baselineï¼‰")
print(classification_report(y_true, y_pred, target_names=list(LABEL_MAP.values())))
